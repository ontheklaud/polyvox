import logging
from io import BytesIO
import os
import sys
import urllib.request

import torch
import intel_extension_for_pytorch as ipex
import scipy.io.wavfile as wavfile
import numpy as np

from DeepLearningExamples.PyTorch.SpeechSynthesis.Tacotron2.tacotron2 import model as tacotron2
from DeepLearningExamples.PyTorch.SpeechSynthesis.Tacotron2.waveglow import model as waveglow

import streamlit as st


# from,
# - https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/SpeechSynthesis/Tacotron2/tacotron2/entrypoints.py
# - https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/SpeechSynthesis/Tacotron2/waveglow/entrypoints.py
def _download_checkpoint(checkpoint, force_reload, model_dir: str = ""):
    if model_dir == "":
        model_dir = os.path.join(torch.hub._get_torch_home(), 'checkpoints')
    else:
        pass
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)
    ckpt_file = os.path.join(model_dir, os.path.basename(checkpoint))
    if not os.path.exists(ckpt_file) or force_reload:
        sys.stderr.write('Downloading checkpoint from {}\n'.format(checkpoint))
        urllib.request.urlretrieve(checkpoint, ckpt_file)
    return ckpt_file


# from https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/SpeechSynthesis/Tacotron2/inference.py
def checkpoint_from_distributed(state_dict):
    """
    Checks whether checkpoint was generated by DistributedDataParallel. DDP
    wraps model in additional "module.", it needs to be unwrapped for single
    GPU inference.
    :param state_dict: model's state dict
    """
    ret = False
    for key, _ in state_dict.items():
        if key.find('module.') != -1:
            ret = True
            break
    return ret


# from https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/SpeechSynthesis/Tacotron2/inference.py
def unwrap_distributed(state_dict):
    """
    Unwraps model from DistributedDataParallel.
    DDP wraps model in additional "module.", it needs to be removed for single
    GPU inference.
    :param state_dict: model's state dict
    """
    new_state_dict = {}
    for key, value in state_dict.items():
        new_key = key.replace('module.1.', '')
        new_key = new_key.replace('module.', '')
        new_state_dict[new_key] = value
    return new_state_dict


# from https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/SpeechSynthesis/Tacotron2/tacotron2/entrypoints.py
def nvidia_tts_utils_for_xpu():
    class Processing:

        from DeepLearningExamples.PyTorch.SpeechSynthesis.Tacotron2.tacotron2.text import text_to_sequence

        @staticmethod
        def pad_sequences(batch):
            # Right zero-pad all one-hot text sequences to max input length
            input_lengths, ids_sorted_decreasing = torch.sort(
                torch.LongTensor([len(x) for x in batch]),
                dim=0, descending=True)
            max_input_len = input_lengths[0]

            text_padded = torch.LongTensor(len(batch), max_input_len)
            text_padded.zero_()
            for i in range(len(ids_sorted_decreasing)):
                text = batch[ids_sorted_decreasing[i]]
                text_padded[i, :text.size(0)] = text

            return text_padded, input_lengths

        @staticmethod
        def prepare_input_sequence(texts, cpu_run=False, device="cpu"):

            d = []
            for i, text in enumerate(texts):
                d.append(torch.IntTensor(
                    Processing.text_to_sequence(text, ['english_cleaners'])[:]))

            text_padded, input_lengths = Processing.pad_sequences(d)
            # if not cpu_run:
            #     text_padded = text_padded.cuda().long()
            #     input_lengths = input_lengths.cuda().long()
            # else:
            #     text_padded = text_padded.long()
            #     input_lengths = input_lengths.long()

            text_padded = text_padded.to(device).long()
            input_lengths = input_lengths.to(device).long()

            return text_padded, input_lengths

    return Processing()


def main():

    #checkpoint_tacotron2_fp16 = 'https://api.ngc.nvidia.com/v2/models/nvidia/tacotron2_pyt_ckpt_amp/versions/19.09.0/files/nvidia_tacotron2pyt_fp16_20190427'
    #checkpoint_tacotron2_fp32 = 'https://api.ngc.nvidia.com/v2/models/nvidia/tacotron2_pyt_ckpt_fp32/versions/19.09.0/files/nvidia_tacotron2pyt_fp32_20190427'
    #ckpt_file_tacotron2_fp16 = _download_checkpoint(checkpoint_tacotron2_fp16, True)
    #ckpt_file_tacotron2_fp32 = _download_checkpoint(checkpoint_tacotron2_fp32, True)

    tacotron2_ckpt = torch.load("assets/nvidia_tacotron2pyt_fp32_20190427", map_location=torch.device('cpu'))
    tacotron2_state_dict = tacotron2_ckpt['state_dict']
    if checkpoint_from_distributed(tacotron2_state_dict):
        tacotron2_state_dict = unwrap_distributed(tacotron2_state_dict)
    tacotron2_config = tacotron2_ckpt['config']

    tacotron2_m = tacotron2.Tacotron2(**tacotron2_config)
    tacotron2_m.load_state_dict(tacotron2_state_dict)
    tacotron2_m.eval()
    tacotron2_m.to(torch.device('xpu'))

    #checkpoint_waveglow_fp16 = 'https://api.ngc.nvidia.com/v2/models/nvidia/waveglow_ckpt_amp/versions/19.09.0/files/nvidia_waveglowpyt_fp16_20190427'
    #checkpoint_waveglow_fp32 = 'https://api.ngc.nvidia.com/v2/models/nvidia/waveglow_ckpt_fp32/versions/19.09.0/files/nvidia_waveglowpyt_fp32_20190427'
    #ckpt_file_waveglow_fp16 = _download_checkpoint(checkpoint_waveglow_fp16, True, "assets")
    #ckpt_file_waveglow_fp32 = _download_checkpoint(checkpoint_waveglow_fp32, True, "assets")

    waveglow_ckpt = torch.load("assets/nvidia_waveglowpyt_fp32_20190427", map_location=torch.device('cpu'))
    waveglow_state_dict = waveglow_ckpt['state_dict']
    if checkpoint_from_distributed(waveglow_state_dict):
        waveglow_state_dict = unwrap_distributed(waveglow_state_dict)
    waveglow_config = waveglow_ckpt['config']

    waveglow_m = waveglow.WaveGlow(**waveglow_config)
    waveglow_m.load_state_dict(waveglow_state_dict)
    waveglow_m.eval()
    waveglow_m.to(torch.device('xpu'))

    # text utils
    utils = nvidia_tts_utils_for_xpu()

    # Streamlit UI - title and description
    st.title("Tacotron 2 TTS")
    st.write("Enter text below to generate speech:")

    # Input text from the user
    input_text = st.text_input("Input Text", "Hello, welcome to Tacotron 2!")

    # Check if input text is provided and ready for processing
    if st.button("Generate"):
        if input_text.strip():
            st.success(f"Processing text: '{input_text}'")

            sequences, lengths = utils.prepare_input_sequence([input_text], device='xpu')

            with torch.no_grad():
                mel, _, _ = tacotron2_m.infer(sequences, lengths)

                audio = waveglow_m.infer(mel)

                audio_post = audio.squeeze().cpu().numpy()

                # Generate WAV in memory
                audio_buffer = BytesIO()
                wavfile.write(audio_buffer,
                              rate=22050,
                              data=(audio_post * 32767).astype(np.int16))  # Scale and convert to 16-bit PCM
                audio_buffer.seek(0)

                # Pass to Streamlit
                st.audio(audio_buffer, format="audio/wav")
                st.success("Audio generated successfully!")

        else:
            st.error("Please provide some text for synthesis.")

    # fin
    return


if __name__ == "__main__":
    main()
